
ID: h8k1jur8,
Text: does non covid lung lesion help investigating transferability in covid ct image segmentation coronavirus disease covid is a highly contagious virus spreading all around the world deep learning has been adopted as an effective technique to aid covid detection and segmentation from computed tomography ct images the major challenge lies in the inadequate public covid datasets recently transfer learning has become a widely used technique that leverages the knowledge gained while solving one problem and applying it to a different but related problem however it remains unclear whether various non covid lung lesions could contribute to segmenting covid infection areas and how to better conduct this transfer procedure this paper provides a way to understand the transferability of non covid lung lesions based on a publicly available covid ct dataset and three public non covid datasets we evaluate four transfer learning methods using d u net as a standard encoder decoder method the results reveal the benefits of transferring knowledge from non covid lung lesions and learning from multiple lung lesion datasets can extract more general features leading to accurate and robust pre trained models we further show the capability of the encoder to learn feature representations of lung lesions which improves segmentation accuracy and facilitates training convergence in addition our proposed multi encoder learning method incorporates transferred lung lesion features from non covid datasets effectively and achieves significant improvement these findings promote new insights into transfer learning for covid ct image segmentation which can also be further generalized to other medical tasks

ID: r0j0368k,
Text: covid mobility data collection of seoul south korea the relationship between pandemic and human mobility has received considerable attention from scholars as investigating such relationship can provide an indication of how human mobility changes in response to a public health crisis or whether reduced mobility contributes to preventing the spread of an infectious disease while several studies attempted to unveil this relationship no studies have focused on changes in mobility pattern at a finer scale utilizing high resolution datasets to address the complex association between pandemic s spread and human mobility this paper presents two categories of mobility datasets trip mode and trip purpose that concern nearly million citizens movements during the first days of covid in seoul south korea where no major lockdown has been imposed we curate hourly data of subway ridership traffic volume and population present count at selected points of interests the results to be derived from the presented datasets can be used as an important reference for public health decision making in the post covid era

ID: vsl94lhz,
Text: twitter sentiment classification for measuring public health concerns an important task of public health officials is to keep track of health issues such as spreading epidemics in this paper we are addressing the issue of spreading public concern about epidemics public concern about a communicable disease can be seen as a problem of its own keeping track of trends in concern about public health and identifying peaks of public concern are therefore crucial tasks however monitoring public health concerns is not only expensive with traditional surveillance systems but also suffers from limited coverage and significant delays to address these problems we are using twitter messages which are available free of cost are generated world wide and are posted in real time we are measuring public concern using a two step sentiment classification approach in the first step we distinguish personal tweets from news i e non personal tweets in the second step we further separate personal negative from personal non negative tweets both these steps consist themselves of two sub steps in the first sub step of both steps our programs automatically generate training data using an emotion oriented clue based method in the second sub step we are training and testing three different machine learning ml models with the training data from the first sub step this allows us to determine the best ml model for different datasets furthermore we are testing the already trained ml models with a human annotated disjoint dataset based on the number of tweets classified as personal negative we compute a measure of concern moc and a timeline of the moc we attempt to correlate peaks of the moc timeline to peaks of the news non personal timeline our best accuracy results are achieved using the two step method with a naïve bayes classifier for the epidemic domain six datasets and the mental health domain three datasets

ID: yoav2b35,
Text: policycloud analytics as a service facilitating efficient data driven public policy management while several application domains are exploiting the added value of analytics over various datasets to obtain actionable insights and drive decision making the public policy management domain has not yet taken advantage of the full potential of the aforementioned analytics and data models diverse and heterogeneous datasets are being generated from various sources which could be utilized across the complete policies lifecycle i e modelling creation evaluation and optimization to realize efficient policy management to this end in this paper we present an overall architecture of a cloud based environment that facilitates data retrieval and analytics as well as policy modelling creation and optimization the environment enables data collection from heterogeneous sources linking and aggregation complemented with data cleaning and interoperability techniques in order to make the data ready for use an innovative approach for analytics as a service is introduced and linked with a policy development toolkit which is an integrated web based environment to fulfil the requirements of the public policy ecosystem stakeholders

ID: 5ll60v8p,
Text: statistical explorations and univariate timeseries analysis on covid datasets to understand the trend of disease spreading and death severe acute respiratory syndrome coronavirus sars cov the novel coronavirus is responsible for the ongoing worldwide pandemic world health organization who assigned an international classification of diseases icd code covid as the name of the new disease coronaviruses are generally transferred by people and many diverse species of animals including birds and mammals such as cattle camels cats and bats infrequently the coronavirus can be transferred from animals to humans and then propagate among people such as with middle east respiratory syndrome mers cov severe acute respiratory syndrome sars cov and now with this new virus namely sars cov or human coronavirus its rapid spreading has sent billions of people into lockdown as health services struggle to cope up the covid outbreak comes along with an exponential growth of new infections as well as a growing death count a major goal to limit the further exponential spreading is to slow down the transmission rate which is denoted by a spread factor f and we proposed an algorithm in this study for analyzing the same this paper addresses the potential of data science to assess the risk factors correlated with covid after analyzing existing datasets available in ourworldindata org oxford university database and newly simulated datasets following the analysis of different univariate long short term memory lstm models for forecasting new cases and resulting deaths the result shows that vanilla stacked and bidirectional lstm models outperformed multilayer lstm models besides we discuss the findings related to the statistical analysis on simulated datasets for correlation analysis we included features such as external temperature rainfall sunshine population infected cases death country population area and population density of the past three months january february and march in for univariate timeseries forecasting using lstm we used datasets from january to april

ID: 30ri8v61,
Text: statistical explorations and univariate timeseries analysis on covid datasets to understand the trend of disease spreading and death severe acute respiratory syndrome coronavirus sars cov the novel coronavirus is responsible for the ongoing worldwide pandemic world health organization who assigned an international classification of diseases icd code covid as the name of the new disease coronaviruses are generally transferred by people and many diverse species of animals including birds and mammals such as cattle camels cats and bats infrequently the coronavirus can be transferred from animals to humans and then propagate among people such as with middle east respiratory syndrome mers cov severe acute respiratory syndrome sars cov and now with this new virus namely sars cov or human coronavirus its rapid spreading has sent billions of people into lockdown as health services struggle to cope up the covid outbreak comes along with an exponential growth of new infections as well as a growing death count a major goal to limit the further exponential spreading is to slow down the transmission rate which is denoted by a spread factor f and we proposed an algorithm in this study for analyzing the same this paper addresses the potential of data science to assess the risk factors correlated with covid after analyzing existing datasets available in ourworldindata org oxford university database and newly simulated datasets following the analysis of different univariate long short term memory lstm models for forecasting new cases and resulting deaths the result shows that vanilla stacked and bidirectional lstm models outperformed multilayer lstm models besides we discuss the findings related to the statistical analysis on simulated datasets for correlation analysis we included features such as external temperature rainfall sunshine population infected cases death country population area and population density of the past three months january february and march in for univariate timeseries forecasting using lstm we used datasets from january to april

ID: gxstlzuk,
Text: genome detective coronavirus typing tool for rapid identification and characterization of novel coronavirus genomes genome detective is a web based user friendly software application to quickly and accurately assemble all known virus genomes from next generation sequencing datasets this application allows the identification of phylogenetic clusters and genotypes from assembled genomes in fasta format since its release in we have produced a number of typing tools for emergent viruses that have caused large outbreaks such as zika and yellow fever virus in brazil here we present the genome detective coronavirus typing tool that can accurately identify novel coronavirus ncov sequences isolated in china and around the world the tool can accept up to sequences per submission and the analysis of a new whole genome sequence will take approximately one minute the tool has been tested and validated with hundreds of whole genomes from ten coronavirus species and correctly classified all of the sars related coronavirus sarsr cov and all of the available public data for ncov the tool also allows tracking of new viral mutations as the outbreak expands globally which may help to accelerate the development of novel diagnostics drugs and vaccines

ID: 6uaj8fb7,
Text: highly ace expression in pancreas may cause pancreas damage after sars cov infection the ongoing outbreak of coronavirus disease covid caused by severe acute respiratory syndrome coronavirus sars cov started in the end of in china has triggered a global public health crisis previous studies have shown that sars cov infects cells by binding angiotensin converting enzyme ace which is the same as sars cov the expression and distribution of ace in the pancreas are unknown at the same time the injury of pancreas after sars cov infection has not been concerned here we collected public datasets bulk rna seq and single cell rna seq to indicate the expression and the distribution of ace in pancreas in both exocrine glands and islets and further clinical data including mild and severe patients with covid demonstrated there existed mild pancreatitis in the severe cases patients showed elevated levels of both amylase and lipase and patients showed imaging alterations only one patient showed elevated levels of both amylase and lipase in mild cases without imaging changes our study revealed the phenomenon and possible cause of mild pancreatic injury in patients with covid this suggests that pancreatitis after sars cov infection should also be paid attention in clinical work

ID: g3sq5j6k,
Text: a transcriptional regulatory atlas of coronavirus infection of human cells identifying transcriptional responses that are most consistently associated with experimental coronavirus cov infection can help illuminate human cellular signaling pathways impacted by cov infection here we distilled over data points from publically archived cov infection transcriptomic datasets into consensus regulatory signatures or consensomes that rank genes based on their transcriptional responsiveness to infection of human cells by mers sars cov and sars cov subtypes we computed overlap between genes with elevated rankings in the cov consensomes against those from transcriptomic and chip seq consensomes for nearly cellular signaling pathway nodes validating the cov infection consensomes we identified robust overlap between their highly ranked genes and high confidence targets of signaling pathway nodes with known roles in cov infection we then developed a series of use cases that illustrate the utility of the cov consensomes for hypothesis generation around mechanistic aspects of the cellular response to cov infection we make the cov infection datasets and their universe of underlying data points freely accessible through the signaling pathways project web knowledgebase at https www signalingpathways org datasets index jsf

ID: ns628u21,
Text: alpha satellite an ai driven system and benchmark datasets for hierarchical community level risk assessment to help combat covid the novel coronavirus and its deadly outbreak have posed grand challenges to human society as of march there have been confirmed cases and reported deaths in the united states and the world health organization who characterized coronavirus disease covid which has infected more than people with more than deaths in at least countries a global pandemic a growing number of areas reporting local sub national community transmission would represent a significant turn for the worse in the battle against the novel coronavirus which points to an urgent need for expanded surveillance so we can better understand the spread of covid and thus better respond with actionable strategies for community mitigation by advancing capabilities of artificial intelligence ai and leveraging the large scale and real time data generated from heterogeneous sources e g disease related data from official public health organizations demographic data mobility data and user geneated data from social media in this work we propose and develop an ai driven system named alpha satellite as an initial offering to provide hierarchical community level risk assessment to assist with the development of strategies for combating the fast evolving covid pandemic more specifically given a specific location either user input or automatic positioning the developed system will automatically provide risk indexes associated with it in a hierarchical manner e g state county city specific location to enable individuals to select appropriate actions for protection while minimizing disruptions to daily life to the extent possible the developed system and the generated benchmark datasets have been made publicly accessible through our website the system description and disclaimer are also available in our website

ID: c9niqu56,
Text: simulating the spread of epidemics in china on the multi layer transportation network beyond the coronavirus in wuhan based on the seir model and the modeling of urban transportation networks a general purpose simulator for the spread of epidemics in chinese cities is built the chinese public transportation system between over prefectural level cities is modeled as a multi layer bi partite network with layers representing different means of transportation airlines railways sail routes and buses and nodes divided into two categories central cities peripheral cities at each city an open system seir model tracks the local spread of the disease with population in and out flow exchanging with the overlying transportation network the model accounts for different transmissivities of the epidemic on different transportation media the transit of inbound flow at cities cross infection on public transportation vehicles due to path overlap and the realistic considerations that the infected population are not entering public transportation and the recovered population are not subject to repeated infections the model could be used to simulate the city level spread in china and potentially other countries of an arbitrary epidemic characterized by its basic reproduction number incubation period infection period and zoonotic force originated from any chinese prefectural level city s during the period before effective government interventions are implemented flowmaps are input into the system to trigger inter city dynamics assuming different flow strength determined from empirical observation within between the bi partite divisions of nodes the model is used to simulate the coronavirus epidemic in wuhan it shows that the framework is robust and reliable and simulated results match public city level datasets to an extraordinary extent

ID: 12l3t9zh,
Text: maskit masking for efficient utilization of incomplete public datasets for training deep learning models a major challenge in training deep learning models is the lack of high quality and complete datasets in the paper we present a masking approach for training deep learning models from a publicly available but incomplete dataset for example city of hamburg germany maintains a list of trees along the roads but this dataset does not contain any information about trees in private homes and parks to train a deep learning model on such a dataset we mask the street trees and aerial images with the road network road network used for creating the mask is downloaded from openstreetmap and it marks the area where the training data is available the mask is passed to the model as one of the inputs and it also coats the output our model learns to successfully predict trees only in the masked region with accuracy

ID: oid5bok9,
Text: masked face recognition dataset and application in order to effectively prevent the spread of covid virus almost everyone wears a mask during coronavirus epidemic this almost makes conventional facial recognition technology ineffective in many cases such as community access control face access control facial attendance facial security checks at train stations etc therefore it is very urgent to improve the recognition performance of the existing face recognition technology on the masked faces most current advanced face recognition approaches are designed based on deep learning which depend on a large number of face samples however at present there are no publicly available masked face recognition datasets to this end this work proposes three types of masked face datasets including masked face detection dataset mfdd real world masked face recognition dataset rmfrd and simulated masked face recognition dataset smfrd among them to the best of our knowledge rmfrd is currently theworld s largest real world masked face dataset these datasets are freely available to industry and academia based on which various applications on masked faces can be developed the multi granularity masked face recognition model we developed achieves accuracy exceeding the results reported by the industry our datasets are available at https github com x zhangyang real world masked face dataset

ID: argclxp1,
Text: genomic analysis and geographic visualization of h n and sars cov emerging infectious diseases and organisms present critical issues of national security public health and economic welfare we still understand little about the zoonotic potential of many viruses to this end we are developing novel database tools to manage comparative genomic datasets these tools add value because they allow us to summarize the direction frequency and order of genomic changes we will perform numerous real world tests with our tools with both avian influenza and coronaviruses

ID: q34w89fa,
Text: oscillations in usa covid incidence and mortality data reflect societal factors the covid pandemic currently in process differs from other infectious disease calamities that have previously plagued humanity in the vast amount of information that is produces each day which includes daily estimates of the disease incidence and mortality data apart from providing actionable information to public health authorities on the trend of the pandemic the daily incidence reflects the process of disease in a susceptible population and thus reflects the pathogenesis of covid the public health response and diagnosis and reporting both daily new cases and daily mortality data in the us exhibit periodic oscillatory patterns by analyzing nyc and la testing data we demonstrate that this oscillation in the number of cases can be strongly explained by the daily variation in testing this seems to rule out alternative hypotheses such as increased infections on certain days of the week as driving this oscillation similarly we show that the apparent oscillation in mortality in the us data is mostly an artifact of reporting which disappears in datasets that record death by episode date such as the nyc and la datasets periodic oscillations in covid incidence and mortality data reflect testing and reporting practices and contingencies thus these contingencies should be considered first prior to suggesting social or biological mechanisms

ID: joacrild,
Text: developing a supervised learning based social media business sentiment index the fast growing digital data generation leads to the emergence of the era of big data which become particularly more valuable because approximately of the collected data in the world comes from social media thus the investigation of online social network services is of paramount importance in this paper we use the sentiment analysis which detects attitudes and emotions toward issues of society posted in social media to understand the actual economic situation to this end two steps are suggested in the first step after training the sentiment classifiers with several big data sources of social media datasets we consider three types of feature sets feature vector sequence vector and a combination of dictionary based feature and sequence vectors then the performance of six classifiers is assessed maxent l c decision tree svm kernel ada boost naïve bayes and maxent in the second step we collect datasets that are relevant to several economic words that the public use to explicitly express their opinions finally we use a vector auto regression analysis to confirm our hypothesis the results show the statistically significant relationship between public sentiment and economic performance that is depression and unemployment lead to kospi also it shows that the extracted keywords from the sentiment analysis such as price year end tax and budget deficit cause the exchange rates

ID: zixj2u7c,
Text: using informatics to guide public health policy during the covid pandemic in the usa background current and future pandemics will require informatics solutions to assess the risks resources and policies to guide better public health decision making methods cross sectional study of all covid cases and deaths in the usa on a population and resource adjusted basis as of april by applying biomedical informatics and data visualization tools to several public and federal government datasets including analysis of the impact of statewide stay at home orders results there were cases and deaths per million residents respectively in the usa with variable distributions throughout divisions regions and states forty two states and washington dc had statewide stay at home orders with the remaining states having population adjusted characteristics in the highest risk quartile conclusions effective national preparedness requires clearly understanding states ability to predict manage and balance public health needs through all stages of a pandemic this will require leveraging data quickly correctly and responsibly into sound public health policies

ID: vpcx2t3w,
Text: the digestive system is a potential route of ncov infection a bioinformatics analysis based on single cell transcriptomes since december a newly identified coronavirus novel coronavirus ncov is causing outbreak of pneumonia in one of largest cities wuhan in hubei province of china and has draw significant public health attention the same as severe acute respiratory syndrome coronavirus sars cov ncov enters into host cells via cell receptor angiotensin converting enzyme ii ace in order to dissect the ace expressing cell composition and proportion and explore a potential route of the ncov infection in digestive system infection datasets with single cell transcriptomes of lung esophagus gastric ileum and colon were analyzed the data showed that ace was not only highly expressed in the lung at cells esophagus upper and stratified epithelial cells but also in absorptive enterocytes from ileum and colon these results indicated along with respiratory systems digestive system is a potential routes for ncov infection in conclusion this study has provided the bioinformatics evidence of the potential route for infection of ncov in digestive system along with respiratory tract and may have significant impact for our healthy policy setting regards to prevention of ncov infection

ID: xn12s005,
Text: unsupervised method based on superpixel segmentation for corpus callosum parcellation in mri scans in this paper we introduce an unsupervised method for the parcellation of the corpus callosum cc from mri images since there are no visible landmarks within the structure that explicit its parcels non geometric cc parcellation is a challenging task especially that almost of proposed methods are geometric or data based in fact in order to subdivide the cc from brain sagittal mri scans we adopt the probabilistic neural network as a clustering technique then we use a cluster validity measure based on the maximum entropy vmep to obtain the optimal number of classes after that we obtain the isolated cc that we parcel automatically using slic simple linear iterative clustering as superpixel segmentation technique the obtained results on two challenging public datasets prove the performance of the proposed method against geometric methods from the state of the art indeed as best as we know it is the first work that investigates the validation of a cc parcellation method on ground truth datasets using many objective metrics

ID: x4clpzte,
Text: covid automatic detection from x ray images utilizing transfer learning with convolutional neural networks in this study a dataset of x ray images from patients with common bacterial pneumonia confirmed covid disease and normal incidents was utilized for the automatic detection of the coronavirus disease the aim of the study is to evaluate the performance of state of the art convolutional neural network architectures proposed over the recent years for medical image classification specifically the procedure called transfer learning was adopted with transfer learning the detection of various abnormalities in small medical image datasets is an achievable target often yielding remarkable results the datasets utilized in this experiment are two firstly a collection of x ray images including images with confirmed covid disease images with confirmed common bacterial pneumonia and images of normal conditions secondly a dataset including images with confirmed covid disease images with confirmed bacterial and viral pneumonia and images of normal conditions the data was collected from the available x ray images on public medical repositories the results suggest that deep learning with x ray imaging may extract significant biomarkers related to the covid disease while the best accuracy sensitivity and specificity obtained is and respectively since by now all diagnostic tests show failure rates such as to raise concerns the probability of incorporating x rays into the diagnosis of the disease could be assessed by the medical community based on the findings while more research to evaluate the x ray approach from different aspects may be conducted

ID: 3iu8qx7n,
Text: using informatics to guide public health policy during the covid pandemic in the usa background current and future pandemics will require informatics solutions to assess the risks resources and policies to guide better public health decision making methods cross sectional study of all covid cases and deaths in the usa on a population and resource adjusted basis as of april by applying biomedical informatics and data visualization tools to several public and federal government datasets including analysis of the impact of statewide stay at home orders results there were cases and deaths per million residents respectively in the usa with variable distributions throughout divisions regions and states forty two states and washington dc had statewide stay at home orders with the remaining states having population adjusted characteristics in the highest risk quartile conclusions effective national preparedness requires clearly understanding states ability to predict manage and balance public health needs through all stages of a pandemic this will require leveraging data quickly correctly and responsibly into sound public health policies

ID: st5idleq,
Text: genome detective coronavirus typing tool for rapid identification and characterization of novel coronavirus genomes summary genome detective is a web based user friendly software application to quickly and accurately assemble all known virus genomes from next generation sequencing datasets this application allows the identification of phylogenetic clusters and genotypes from assembled genomes in fasta format since its release in we have produced a number of typing tools for emergent viruses that have caused large outbreaks such as zika and yellow fever virus in brazil here we present the genome detective coronavirus typing tool that can accurately identify the novel severe acute respiratory syndrome sars related coronavirus sars cov sequences isolated in china and around the world the tool can accept up to sequences per submission and the analysis of a new whole genome sequence will take approximately one minute the tool has been tested and validated with hundreds of whole genomes from ten coronavirus species and correctly classified all of the sars related coronavirus sarsr cov and all of the available public data for sars cov the tool also allows tracking of new viral mutations as the outbreak expands globally which may help to accelerate the development of novel diagnostics drugs and vaccines to stop the covid disease availability https www genomedetective com app typingtool cov supplementary information

ID: dbwfn27p,
Text: single cell rna expression profiling of ace the putative receptor of wuhan ncov in the nasal tissue a novel coronavirus ncov was first identified in wuhan hubei province and then spreads to the other provinces of china who decides to determine a public health emergency of international concern pheic of ncov ncov was reported to share the same receptor angiotensin converting enzyme ace with sars cov here based on the public single cell rna seq datasets we analyzed the ace rna expression profile in the tissues at different locations of the respiratory tract the result indicates that the ace expression appears in nasal epithelial cells we found that the size of this population of ace expressing nasal epithelial cells is comparable with the size of the population of ace expression type ii alveolar cells at in the asian sample reported by yu zhao et al we further detected ncov by polymerase chain reaction pcr from the nasal swab and throat swab of seven suspected cases we found that ncov tends to have a higher concentration in the nasal swab comparing to the throat swab which could attribute to the ace expressing nasal epithelial cells we hope this study could be informative for virus prevention strategy development especially the treatment of nasal mucus

ID: csokkcqq,
Text: evaluating performance of metagenomic characterization algorithms using in silico datasets generated with fastqsim background in silico bacterial viral and human truth datasets were generated to evaluate available metagenomics algorithms sequenced datasets include background organisms creating ambiguity in the true source organism for each read bacterial and viral datasets were created with even and staggered coverage to evaluate organism identification read mapping and gene identification capabilities of available algorithms these truth datasets are provided as a resource for the development and refinement of metagenomic algorithms algorithm performance on these truth datasets can inform decision makers on strengths and weaknesses of available algorithms and how the results may be best leveraged for bacterial and viral organism identification and characterization source organisms were selected to mirror communities described in the human microbiome project as well as the emerging pathogens listed by the national institute of allergy and infectious diseases the six in silico datasets were used to evaluate the performance of six leading metagenomics algorithms metascope kraken lmat metaphlan metacv and metaphyler results algorithms were evaluated on runtime true positive organisms identified to the genus and species levels false positive organisms identified to genus and species level read mapping relative abundance estimation and gene calling no algorithm out performed the others in all categories and the algorithm or algorithms of choice strongly depends on analysis goals metaphlan excels for bacteria and lmat for viruses the algorithms were ranked by overall performance using a normalized weighted sum of the above metrics and metascope emerged as the overall winner followed by kraken and lmat conclusions simulated fastq datasets with well characterized truth data about microbial community composition reveal numerous insights about the relative strengths and weaknesses of the metagenomics algorithms evaluated the simulated datasets are available to download from the sequence read archive srp

ID: kfy7v56x,
Text: analyzing the epidemiological outbreak of covid a visual exploratory data analysis approach there is an obvious concern globally regarding the fact about the emerging coronavirus novel coronavirus ncov as a worldwide public health threat as the outbreak of covid causes by the severe acute respiratory syndrome coronavirus sars cov progresses within china and beyond rapidly available epidemiological data are needed to guide strategies for situational awareness and intervention the recent outbreak of pneumonia in wuhan china caused by the sars cov emphasizes the importance of analyzing the epidemiological data of this novel virus and predicting their risks of infecting people all around the globe in this study we present an effort to compile and analyze epidemiological outbreak information on covid based on the several open datasets on ncov provided by the johns hopkins university world health organization chinese center for disease control and prevention national health commission and dxy an exploratory data analysis with visualizations has been made to understand the number of different cases reported confirmed death and recovered in different provinces of china and outside of china overall at the outset of an outbreak like this it is highly important to readily provide information to begin the evaluation necessary to understand the risks and begin containment activities

ID: 95o2v09d,
Text: google dataset search by the numbers scientists governments and companies increasingly publish datasets on the web google s dataset search extracts dataset metadata expressed using schema org and similar vocabularies from web pages in order to make datasets discoverable since we started the work on dataset search in the number of datasets described in schema org has grown from about k to almost m thus this corpus has become a valuable snapshot of data on the web to the best of our knowledge this corpus is the largest and most diverse of its kind we analyze this corpus and discuss where the datasets originate from what topics they cover which form they take and what people searching for datasets are interested in based on this analysis we identify gaps and possible future work to help make data more discoverable

ID: ef97jzc4,
Text: deep knowledge tracing with transformers in this work we propose a transformer based model to trace students knowledge acquisition we modified the transformer structure to utilize the association between questions and skills and the elapsed time between question steps the use of question skill associations allows the model to learn specific representation for frequently encountered questions while representing rare questions with their underline skill representations the inclusion of elapsed time opens the opportunity to address forgetting our approach outperforms the state of the art methods in the literature by roughly in auc with frequently used public datasets

ID: keaxietu,
Text: understanding economic and health factors impacting the spread of covid disease the rapid spread of the coronavirus disease covid had drastically impacted life all over the world while some economies are actively recovering from this pestilence others are experiencing fast and consistent disease spread compelling governments to impose social distancing measures that have put a halt on routines especially in densely populated areas aiming at bringing more light on key economic and public health factors affecting the disease spread this initial study utilizes a quantitative statistical analysis based on the most recent publicly available covid datasets the study had shown and explained multiple significant relationships between the covid data and other country level statistics we have also identified and statistically profiled four major country level clusters with relation to different aspects of covid development and country level economic and health indicators specifically this study has identified potential covid under reporting traits as well as various economic factors that impact covid diagnosis reporting and treatment based on the country clusters we have also described the four disease development scenarios which are tightly knit to country level economic and public health factors finally we have highlighted the potential limitation of reporting and measuring covid and provided recommendations on further in depth quantitative research

ID: rql9fjug,
Text: analyzing the epidemiological outbreak of covid a visual exploratory data analysis approach there is an obvious concern globally regarding the fact about the emerging coronavirus novel coronavirus ncov as a worldwide public health threat as the outbreak of covid causes by the severe acute respiratory syndrome coronavirus sars cov progresses within china and beyond rapidly available epidemiological data are needed to guide strategies for situational awareness and intervention the recent outbreak of pneumonia in wuhan china caused by the sars cov emphasizes the importance of analyzing the epidemiological data of this novel virus and predicting their risks of infecting people all around the globe in this study we present an effort to compile and analyze epidemiological outbreak information on covid based on the several open datasets on ncov provided by the johns hopkins university world health organization chinese center for disease control and prevention national health commission and dxy an exploratory data analysis with visualizations has been made to understand the number of different cases reported confirmed death and recovered in different provinces of china and outside of china overall at the outset of an outbreak like this it is highly important to readily provide information to begin the evaluation necessary to understand the risks and begin containment activities

ID: zrqp24gb,
Text: major infection events over years how is media coverage influencing online information needs of health care professionals and the public background the last decade witnessed turbulent events in public health emerging infections increase of antimicrobial resistance deliberately released threats and ongoing battles with common illnesses were amplified by the spread of disease through increased international travel the internet has dramatically changed the availability of information about outbreaks however little research has been done in comparing the online behavior of public and professionals around the same events and the effect of media coverage of outbreaks on information needs objective to investigate professional and public online information needs around major infection outbreaks and correlate these with media coverage questions include how do health care professionals online needs for public health and infection control information differ from those of the public does dramatic media coverage of outbreaks contribute to the information needs among the public and how do incidents of diseases and major policy events relate to the information needs of professionals methods we used three longitudinal time based datasets from mid until end of a unique record of professional online behavior on uk infection portals national electronic library of infection and national resource of infection control neli nric equivalent public online information needs google trends and relevant media coverage lexisnexis analysis of neli nric logs identified the highest interest around six major infectious diseases clostridium difficile c difficile methicillin resistant staphylococcus aureus mrsa tuberculosis meningitis norovirus and influenza after pre processing the datasets were analyzed and triangulated with each other results public information needs were more static following the actual disease occurrence less than those of professionals whose needs increase with public health events eg mrsa c difficile and the release of major national policies or important documents media coverage of events resulted in major public interest eg the uk outbreak of c difficile mrsa an exception was norovirus showing a seasonal pattern for both public and professionals which matched the periodic disease occurrence meningitis was a clear example of a disease with heightened media coverage tending to focus on individual and celebrity cases influenza was a major concern during the h n outbreak creating massive public interest in line with the spring and autumn peaks in cases although in autumn there was no corresponding increase in media coverage online resources play an increasing role in fulfilling professionals and public information needs conclusions significant factors related to a surge of professional interest around a disease were typically key publications and major policy changes public interests seem more static and correlate with media influence but to a lesser extent than expected the only exception was norovirus exhibiting online public and professional interest correlating with seasonal occurrences of the disease public health agencies with responsibility for risk communication of public health events in particular during outbreaks and emergencies need to collaborate with media in order to ensure the coverage is high quality and evidence based while professionals information needs remain mainly fulfilled by online open access to key resources

ID: emr0eh0i,
Text: identification of a pangolin niche for a ncov like coronavirus via an extensive meta metagenomic search in numerous instances tracking the biological significance of a nucleic acid sequence can be augmented through the identification of environmental niches in which the sequence of interest is present many metagenomic datasets are now available with deep sequencing of samples from diverse biological niches while any individual metagenomic dataset can be readily queried using web based tools meta searches through all such datasets are less accessible in this brief communication we demonstrate such a meta meta genomic approach examining close matches to the wuhan coronavirus ncov in all high throughput sequencing datasets in the ncbi sequence read archive accessible with the keyword virome in addition to the homology to bat coronaviruses observed in descriptions of the ncov sequence f wu et al nature doi org s p zhou et al nature doi org s we note a strong homology to numerous sequence reads in a metavirome dataset generated from the lungs of deceased pangolins reported by liu et al viruses http doi org v our observations are relevant to discussions of the derivation of ncov and illustrate the utility and limitations of meta metagenomic search tools in effective and rapid characterization of potentially significant nucleic acid sequences meta metagenomic searches allow for high speed low cost identification of potentially significant biological niches for sequences of interest

ID: wbhji81g,
Text: exploring a proposed who method to determine thresholds for seasonal influenza surveillance introduction health authorities find thresholds useful to gauge the start and severity of influenza seasons we explored a method for deriving thresholds proposed in an influenza surveillance manual published by the world health organization who methods for we analysed two routine influenza like illness ili datasets general practice sentinel surveillance and a locum medical service sentinel surveillance plus laboratory data and hospital admissions for influenza for each sentinel dataset we created two composite variables from the product of weekly ili data and the relevant laboratory data indicating the proportion of tested specimens that were positive for all datasets including the composite datasets we aligned data on the median week of peak influenza or ili activity and assigned three threshold levels seasonal threshold determined by inspection and two intensity thresholds termed average and alert thresholds determined by calculations of means medians confidence intervals ci and percentiles from the thresholds we compared the seasonal onset end and intensity across all datasets from correlation between datasets was assessed using the mean correlation coefficient results the median week of peak activity was week for all datasets except hospital data week means and medians were comparable and the upper cis were similar to the th percentiles comparison of thresholds revealed variations in defining the start of a season but good agreement in describing the end and intensity of influenza seasons except in hospital admissions data after the pandemic year of the composite variables improved the agreements between the ili and other datasets datasets were well correlated with mean correlation coefficients of for a range of combinations conclusions thresholds for influenza surveillance are easily derived from historical surveillance and laboratory data using the approach proposed by who use of composite variables is helpful for describing influenza season characteristics

ID: tvmytgda,
Text: towards efficient covid ct annotation a benchmark for lung and infection segmentation accurate segmentation of lung and infection in covid ct scans plays an important role in the quantitative management of patients most of the existing studies are based on large and private annotated datasets that are impractical to obtain from a single institution especially when radiologists are busy fighting the coronavirus disease furthermore it is hard to compare current covid ct segmentation methods as they are developed on different datasets trained in different settings and evaluated with different metrics in this paper we created a covid d ct dataset with cases that contains annotated slices and made it publicly available to promote the development of annotation efficient deep learning methods we built three benchmarks for lung and infection segmentation that contain current main research interests e g few shot learning domain generalization and knowledge transfer for a fair comparison among different segmentation methods we also provide unified training validation and testing dataset splits and evaluation metrics and corresponding code in addition we provided more than pre trained baseline models for the benchmarks which not only serve as out of the box segmentation tools but also save computational time for researchers who are interested in covid lung and infection segmentation to the best of our knowledge this work presents the largest public annotated covid ct volume dataset the first segmentation benchmark and the most pre trained models up to now we hope these resources url https gitee com junma covid ct seg benchmark could advance the development of deep learning methods for covid ct segmentation with limited data

ID: 88hswyln,
Text: genome detective coronavirus typing tool for rapid identification and characterization of novel coronavirus genomes summary genome detective is a web based user friendly software application to quickly and accurately assemble all known virus genomes from next generation sequencing datasets this application allows the identification of phylogenetic clusters and genotypes from assembled genomes in fasta format since its release in we have produced a number of typing tools for emergent viruses that have caused large outbreaks such as zika and yellow fever virus in brazil here we present the genome detective coronavirus typing tool that can accurately identify the novel severe acute respiratory syndrome sars related coronavirus sars cov sequences isolated in china and around the world the tool can accept up to sequences per submission and the analysis of a new whole genome sequence will take approximately min the tool has been tested and validated with hundreds of whole genomes from coronavirus species and correctly classified all of the sars related coronavirus sarsr cov and all of the available public data for sars cov the tool also allows tracking of new viral mutations as the outbreak expands globally which may help to accelerate the development of novel diagnostics drugs and vaccines to stop the covid disease availability and implementation https www genomedetective com app typingtool cov contact koen emweb be or deoliveira ukzn ac za supplementary information supplementary data are available at bioinformatics online

ID: k31dagdy,
Text: unveiling covid from chest x ray with deep learning a hurdles race with small data the possibility to use widespread and simple chest x ray cxr imaging for early screening of covid patients is attracting much interest from both the clinical and the ai community in this study we provide insights and also raise warnings on what is reasonable to expect by applying deep learning to covid classification of cxr images we provide a methodological guide and critical reading of an extensive set of statistical results that can be obtained using currently available datasets in particular we take the challenge posed by current small size covid data and show how significant can be the bias introduced by transfer learning using larger public non covid cxr datasets we also contribute by providing results on a medium size covid cxr dataset just collected by one of the major emergency hospitals in northern italy during the peak of the covid pandemic these novel data allow us to contribute to validate the generalization capacity of preliminary results circulating in the scientific community our conclusions shed some light into the possibility to effectively discriminate covid using cxr

ID: sdk0w6vf,
Text: improving multi turn response selection models with complementary last utterance selection by instance weighting open domain retrieval based dialogue systems require a considerable amount of training data to learn their parameters however in practice the negative samples of training data are usually selected from an unannotated conversation data set at random the generated training data is likely to contain noise and affect the performance of the response selection models to address this difficulty we consider utilizing the underlying correlation in the data resource itself to derive different kinds of supervision signals and reduce the influence of noisy data more specially we consider a main complementary task pair the main task i e our focus selects the correct response given the last utterance and context and the complementary task selects the last utterance given the response and context the key point is that the output of the complementary task is used to set instance weights for the main task we conduct extensive experiments in two public datasets and obtain significant improvement in both datasets we also investigate the variant of our approach in multiple aspects and the results have verified the effectiveness of our approach

ID: azxecnaz,
Text: a review on the use of artificial intelligence for medical imaging of the lungs of patients with coronavirus disease the results of research on the use of artificial intelligence ai for medical imaging of the lungs of patients with coronavirus disease covid has been published in various forms in this study we reviewed the ai for diagnostic imaging of covid pneumonia pubmed arxiv medrxiv and google scholar were used to search for ai studies there were studies of covid that used ai for medical imaging of these studies used ai for computed tomography ct and used ai for chest radiography eight studies presented independent test data used disclosed data and disclosed the ai source codes the number of datasets ranged from to with sensitivities ranging from and specificities ranging from for prediction of covid pneumonia four studies with independent test datasets showed a breakdown of the data ratio and reported prediction of covid pneumonia with sensitivity specificity and area under the curve auc these studies showed very high sensitivity specificity and auc in the range of and respectively

ID: 71njvx7l,
Text: improved prediction of mhc class i binders non binders peptides through artificial neural network using variable learning rate sars corona virus a case study fundamental step of an adaptive immune response to pathogen or vaccine is the binding of short peptides also called epitopes to major histocompatibility complex mhc molecules the various prediction algorithms are being used to capture the mhc peptide binding preference allowing the rapid scan of entire pathogen proteomes for peptide likely to bind mhc saving the cost effort and time however the number of known binders non binders bnb to a specific mhc molecule is limited in many cases which still poses a computational challenge for prediction the training data should be adequate to predict bnb using any machine learning approach in this study variable learning rate has been demonstrated for training artificial neural network and predicting bnb for small datasets the approach can be used for large datasets as well the dataset for different mhc class i alleles for sars corona virus tor replicase polyprotein ab has been used for training and prediction of bnb a total of datasets nine different mhc class i alleles with tenfold cross validation have been retrieved from iedb database for bnb for fixed learning rate approach the best value of aroc is and in most of the cases it is which shows the poor predictions in case of variable learning rate of the datasets the value of aroc for datasets is between and and for datasets the value is between and and for rest of datasets it is between and which indicates very good performance in most of the cases

ID: 6t4a9is5,
Text: large protein as a potential target for use in rabies diagnostics rabies is a zoonotic viral disease that remains a serious threat to public health worldwide the rabies lyssavirus rabv genome encodes five structural proteins multifunctional and significant for pathogenicity the large protein l presents well conserved genomic regions which may be a good alternative to generate informative datasets for development of new methods for rabies diagnosis this paper describes the development of a technique for the identification of l protein in several rabv strains from different hosts demonstrating that ms based proteomics is a potential method for antigen identification and a good alternative for rabies diagnosis

ID: 7gd0o0pp,
Text: virallink an integrated workflow to investigate the effect of sars cov on intracellular signalling and regulatory pathways the sars cov pandemic of has mobilised scientists around the globe to research all aspects of the coronavirus virus and its infection for fruitful and rapid investigation of viral pathomechanisms a collaborative and interdisciplinary approach is required therefore we have developed virallink a systems biology workflow which reconstructs and analyses networks representing the effect of viruses on intracellular signalling these networks trace the flow of signal from intracellular viral proteins through their human binding proteins and downstream signalling pathways ending with transcription factors regulating genes differentially expressed upon viral exposure in this way the workflow provides a mechanistic insight from previously identified knowledge of virally infected cells by default the workflow is set up to analyse the intracellular effects of sars cov requiring only transcriptomics counts data as input from the user thus encouraging and enabling rapid multidisciplinary research however the wide ranging applicability and modularity of the workflow facilitates customisation of viral context a priori interactions and analysis methods through a case study of sars cov infected bronchial tracheal epithelial cells we evidence the functionality of the workflow and its ability to identify key pathways and proteins in the cellular response to infection the application of virallink to different viral infections in a cell type specific manner using different available transcriptomics datasets will uncover key mechanisms in viral pathogenesis the workflow is available on github https github com korcsmarosgroup virallink in an easily accessible python wrapper script or as customisable modular r and python scripts author summary collaborative and multidisciplinary science provides increased value for experimental datasets and speeds the process of discovery such ways of working are especially important at present due to the urgency of the sars cov pandemic here we present a systems biology workflow which models the effect of viral proteins on the infected host cell to aid collaborative and multidisciplinary research through integration of gene expression datasets with context specific and context agnostic molecular interaction datasets the workflow can be easily applied to different datasets as they are made available application to diverse sars cov datasets will increase our understanding of the mechanistic details of the infection at a cell type specific level aid drug target discovery and help explain the variety of clinical manifestations of the infection

ID: q4kpbocd,
Text: experience of establishing severe acute respiratory surveillance in the netherlands evaluation and challenges abstract the influenza a h n pandemic prompted the world health organization who to recommend countries to establish a national severe acute respiratory infections sari surveillance system for preparedness and emergency response however setting up or maintaining a robust sari surveillance system has been challenging similar to other countries surveillance data on hospitalisations for sari in the netherlands are still limited in contrast to the robust surveillance data in primary care the objective of this narrative review is to provide an overview evaluation and challenges of already available surveillance systems or datasets in the netherlands which might be used for near real time surveillance of severe respiratory infections seven available surveillance systems or datasets in the netherlands were reviewed the evaluation criteria including data quality timeliness representativeness simplicity flexibility acceptability and stability were based on united states centers for disease control and prevention cdc and european centre for disease prevention and control ecdc guidelines for public health surveillance we added sustainability as additional evaluation criterion the best evaluated surveillance system or dataset currently available for sari surveillance is crude mortality monitoring although it lacks specificity in contrast to influenza like illness ili in primary care there is currently no gold standard for sari surveillance in the netherlands based on our experience with sentinel sari surveillance a fully or semi automated passive surveillance system seems most suited for a sustainable sari surveillance system an important future challenge remains integrating sari surveillance into existing hospital programs in order to make surveillance data valuable for public health as well as hospital quality of care management and individual patient care

ID: pxwogqzl,
Text: a review on the use of artificial intelligence for medical imaging of the lungs of patients with coronavirus disease the results of research on the use of artificial intelligence ai for medical imaging of the lungs of patients with coronavirus disease covid has been published in various forms in this study we reviewed the ai for diagnostic imaging of covid pneumonia pubmed arxiv medrxiv and google scholar were used to search for ai studies there were studies of covid that used ai for medical imaging of these studies used ai for computed tomography ct and used ai for chest radiography eight studies presented independent test data used disclosed data and disclosed the ai source codes the number of datasets ranged from to with sensitivities ranging from and specificities ranging from for prediction of covid pneumonia four studies with independent test datasets showed a breakdown of the data ratio and reported prediction of covid pneumonia with sensitivity specificity and area under the curve auc these studies showed very high sensitivity specificity and auc in the range of and respectively

ID: iuk8wqrj,
Text: using wavelets and spectral methods to study patterns in image classification datasets deep learning models extract before a final classification layer features or patterns which are key for their unprecedented advantageous performance however the process of complex nonlinear feature extraction is not well understood a major reason why interpretation adversarial robustness and generalization of deep neural nets are all open research problems in this paper we use wavelet transformation and spectral methods to analyze the contents of image classification datasets extract specific patterns from the datasets and find the associations between patterns and classes we show that each image can be written as the summation of a finite number of rank patterns in the wavelet space providing a low rank approximation that captures the structures and patterns essential for learning regarding the studies on memorization vs learning our results clearly reveal disassociation of patterns from classes when images are randomly labeled our method can be used as a pattern recognition approach to understand and interpret learnability of these datasets it may also be used for gaining insights about the features and patterns that deep classifiers learn from the datasets

ID: rwgv0973,
Text: domain adaptation in highly imbalanced and overlapping datasets in many machine learning domains datasets are characterized by highly imbalanced and overlapping classes particularly in the medical domain a specific list of symptoms can be labeled as one of various different conditions some of these conditions may be more prevalent than others by several orders of magnitude here we present a novel unsupervised domain adaptation scheme for such datasets the scheme based on a specific type of quantification is designed to work under both label and conditional shifts it is demonstrated on datasets generated from electronic health records and provides high quality results for both quantification and domain adaptation in very challenging scenarios potential benefits of using this scheme in the current covid outbreak for estimation of prevalence and probability of infection are discussed

ID: lnfebjjo,
Text: covidiagnosis net deep bayes squeezenet based diagnosis of the coronavirus disease covid from x ray images the coronavirus disease covid outbreak has a tremendous impact on global health and the daily life of people still living in more than two hundred countries the crucial action to gain the force in the fight of covid is to have powerful monitoring of the site forming infected patients most of the initial tests rely on detecting the genetic material of the coronavirus and they have a poor detection rate with the time consuming operation in the ongoing process radiological imaging is also preferred where chest x rays are highlighted in the diagnosis early studies express the patients with an abnormality in chest x rays pointing to the presence of the covid on this motivation there are several studies cover the deep learning based solutions to detect the covid using chest x rays a part of the existing studies use non public datasets others perform on complicated artificial intelligent ai structures in our study we demonstrate an ai based structure to outperform the existing studies the squeezenet that comes forward with its light network design is tuned for the covid diagnosis with bayesian optimization additive fine tuned hyperparameters and augmented dataset make the proposed network perform much better than existing network designs and to obtain a higher covid diagnosis accuracy

ID: duwcljks,
Text: on statistics probability and entropy of interval valued datasets applying interval valued data and methods researchers have made solid accomplishments in information processing and uncertainty management although interval valued statistics and probability are available for interval valued data current inferential decision making schemes rely on point valued statistic and probabilistic measures mostly to enable direct applications of these point valued schemes on interval valued datasets we present point valued variational statistics probability and entropy for interval valued datasets related algorithms are reported with illustrative examples

ID: wulg3bbb,
Text: candidate genes associated with susceptibility for sars coronavirus assuming that no human had any previously acquired immunoprotection against severe acute respiratory syndrome coronavirus sars cov during the sars outbreak the biological bases for possible difference in individual susceptibility are intriguing however this issue has never been fully elucidated based on the premise that sars patients belonging to a given genotype group having a significantly higher sars infection rate than others would imply that genotype group being more susceptible we make use of a compartmental model describing disease transmission dynamics and clinical and gene data of laboratory confirmed sars patients from chinese han population in taiwan to estimate the infection rates of distinct candidate genotype groups among these sars infected individuals the results show that cxcl aa is always protective whenever it appears but appears rarely and only jointly with either fgl t or ho a while fgl t is associated with higher susceptibility unless combined with cxcl ip aa when jointly is associated with lower susceptibility the novel modeling approach proposed which does not require sizable case and control gene datasets could have important future public health implications in swiftly identifying potential high risk groups associated with being highly susceptible to a particular infectious disease

ID: qaqvg453,
Text: alignment free method for dna sequence clustering using fuzzy integral similarity a larger amount of sequence data in private and public databases produced by next generation sequencing put new challenges due to limitation associated with the alignment based method for sequence comparison so there is a high need for faster sequence analysis algorithms in this study we developed an alignment free algorithm for faster sequence analysis the novelty of our approach is the inclusion of fuzzy integral with markov chain for sequence analysis in the alignment free model the method estimate the parameters of a markov chain by considering the frequencies of occurrence of all possible nucleotide pairs from each dna sequence these estimated markov chain parameters were used to calculate similarity among all pairwise combinations of dna sequences based on a fuzzy integral algorithm this matrix is used as an input for the neighbor program in the phylip package for phylogenetic tree construction our method was tested on eight benchmark datasets and on in house generated datasets s rdna sequences from arbuscular mycorrhizal fungi amf and s rdna sequences of bacterial isolates from plant interior the results indicate that the fuzzy integral algorithm is an efficient and feasible alignment free method for sequence analysis on the genomic scale

ID: g9cccwo1,
Text: the influence of social cognitive factors on personal hygiene practices to protect against influenzas using modelling to compare avian a h n and pandemic a h n influenzas in hong kong background understanding population responses to influenza helps optimize public health interventions relevant theoretical frameworks remain nascent purpose to model associations between trust in information perceived hygiene effectiveness knowledge about the causes of influenza perceived susceptibility and worry and personal hygiene practices phps associated with influenza methods cross sectional household telephone surveys on avian influenza a h n and pandemic influenza a h n gathered comparable data on trust in formal and informal sources of influenza information influenza related knowledge perceived hygiene effectiveness worry perceived susceptibility and phps exploratory factor analysis confirmed domain content while confirmatory factor analysis was used to evaluate the extracted factors the hypothesized model compiled from different theoretical frameworks was optimized with structural equation modelling using the a h n data the optimized model was then tested against the a h n dataset results the model was robust across datasets though corresponding path weights differed trust in formal information was positively associated with perceived hygiene effectiveness which was positively associated with phps in both datasets trust in formal information was positively associated with influenza worry in a h n data and with knowledge of influenza cause in a h n data both variables being positively associated with phps trust in informal information was positively associated with influenza worry in both datasets independent of information trust perceived influenza susceptibility associated with influenza worry worry associated with phps in a h n data only conclusions knowledge of influenza cause and perceived php effectiveness were associated with phps improving trust in formal information should increase phps worry was significantly associated with phps in a h n

ID: osyfqu2i,
Text: ednet a large scale hierarchical dataset in education advances in artificial intelligence in education aied and the ever growing scale of interactive educational systems iess have led to the rise of data driven approaches for knowledge tracing and learning path recommendation unfortunately collecting student interaction data is challenging and costly as a result there is no public large scale benchmark dataset reflecting the wide variety of student behaviors observed in modern iess although several datasets such as assistments junyi academy synthetic and statics are publicly available and widely used they are not large enough to leverage the full potential of state of the art data driven models furthermore the recorded behavior is limited to question solving activities to this end we introduce ednet a large scale hierarchical dataset of diverse student activities collected by santa a multi platform self study solution equipped with an artificial intelligence tutoring system ednet contains interactions from students collected over more than years making it the largest public ies dataset released to date unlike existing datasets ednet records a wide variety of student actions ranging from question solving to lecture consumption to item purchasing also ednet has a hierarchical structure which divides the student actions into different levels of abstractions the features of ednet are domain agnostic allowing ednet to be easily extended to different domains the dataset is publicly released for research purposes we plan to host challenges in multiple aied tasks with ednet to provide a common ground for the fair comparison between different state of the art models and to encourage the development of practical and effective methods
